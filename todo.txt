❌ - nevyreseno
❗ - docasne reseni (warning)
❓ - zvazit jine provedeni
🚧 - in progress
✅ - vyreseno

=========

Minor pro v0.1.X:
- ❌ matrix pretty print
- ✅ zmenit raw pointery na smart pointery
- ✅ vyresit warningy od Clangu
- ✅ pridat const metody pro Matrix.h
- ❌ auto& vs auto&&
- ❌ T& vs T&&
- ❌ CustomLayer
- ❌ Bezparametricke konstruktory layeru?
- ❌ Overridnout operator "<<" u tridy Graph -> vypise strukturu grafu
- ✅ Prejmenovat tridu Graph na tridu Network
- ❌ Predelat nektere public metody (apply elementwise, hadamard) na staticke metody
- ✅ metoda XavierInitialization() uvnitr Matrix.h je definovana pouze pro double typy
    - kdyz se zavola s jinym typem, tak se provede implicitni konverze
- ❌ const verze metod pro funkce
- ❌ predelat Apply u aktivacnich funkci, aby braly jako parametr matici a ne skalar
- ✅ smazat Vector tridu
- ❌ udelat Input layer nedostupny z klient kodu
- ❌ ma AddLayer neco vracet? (spis ne)
- ❌ Preskladat poradi typu metod a properties ve vsech tridach - mit jedno dane poradi
- ❌ Objekty by meli vracet hodnotou ne referenci svoje properties vs. velikost properties (obzvlast tech matic)
- ❌ Pridat leaky ReLU
- ✅ Prejmenovat XavierInitialization na XavierInitialization u Matrix
- ❓ Zrusit u Matrix "*" operator a nahradit ho statickou funkci "dot"
- ❓ Dava XavierInitialization u celociselne matice vubec nejaky smysl?
=========

Major pro 0.1.0:
- ✅ README.MD
- ✅ Dummy input layer (ten, co je tam implicitne vzdy) provede operaci: "input * weights + b". Coz by asi prave tento input dummy layer delat nemel
    - Nejspis by mel proste jen zkopirovat vstup a nic jineho. Tohle pusobi tak, ze o tom user nevi
    - Navrh reseni: trida "DummyInputLayer" jako potomek "IdentityLayer"?
- ✅ nejspis oddelat ApplyFunctionElementWise a radeji primo u funkci mit verze, co berou matice
    - nebo radeji aby ApplyFunctionElementWise brala pointer na funkci
    - vyreseno tak, ze jsem oddelal cele ApplyFunctionElementWise a dal pristup k begin a end iteratorum pro data_
- ✅ pridat namespace pro funkce
- ✅ vector trida jako potomek Matrix
- ✅ network class
    - ✅ sablonove?  
    - ✅ expected output
    - ✅ train()
    - ✅ test()
    - ✅ forward vypocet v grafu
    - ✅ backprop vypocet v grafu
        - ✅ gradient descent
        - ✅ stochastic gradient descent
    - ✅ konstruktor by nemel brat jako parametry input a expected output, to by mel brat az train
    - ✅ train ma vratit dvojici (predictedValues_, loss_)
- ✅ Loss interface
    - ✅ loss funkce
    - ✅ gradient vuci predicted output
- 🚧 layer interface
    - ✅ layer input
    - ✅ layer output
    - ✅ activation function
    - ✅ derivace aktivaci
    - ✅ weights
    - ❌ bias
    - ✅ output size
    - ✅ activations
    - ❌❌ Projit inicializaci matic v LayerInterface, mozna se provadi nejake operaci navic (reshapovani, etc)
        ---> - ❓ dovolit operatoru = u Matrix class, dosazeni matice jinych rozmeru do jine matice?
- ✅ jednotlive vrstvy
    - ✅ uvnitr graph class
    - ✅ stejny typ jako graf
- ✅ activation funkce interface
    - ✅ interface
    - ✅ konkretni aktivacni funkce
    - ✅ derivace aktivacnich funkci
- ❌ previous layer vs. next layer (nazvoslovi v Graph class) 
- ❓ exception u matrix class, kdyz nekdo chce instancovat matici, co ma pocet radku/sloupcu 0 (musi byt aspon 1?)
- ❌ copy konstruktory ke vsemu
- ❌ move konstruktory ke vsemu
- ❌ move assignment operators
- ❌ Loss funkce by mela fungovat vzdy nad stejnym typem? A nemelo by to byt optional pro klienta? 
- ❌ unit testy
- ✅ prejmenovat WrongMatrixDimensionException na MatrixShapeException
- ✅ prejmenovat InvalidIndexException na IndexException
- ❓ vytvorit input/output preprocessor, co vzdy zplacati data, aby to mohla brat Matrix trida?
=========

Major pro 1.0.0: 
- ❌❌❌ Podpora pro vice nez 2D matice ---> implementace tensoru ---> predelani vsech layeru + layer interface + vsech funkci                                            
    ---> templaty s promennym poctem parametru